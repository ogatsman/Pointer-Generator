{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a7229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import threading\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "import razdel\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f22c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, List\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.rnn import LSTMCell\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL\n",
    "from allennlp.data.vocabulary import Vocabulary, DEFAULT_OOV_TOKEN\n",
    "from allennlp.modules import TextFieldEmbedder, Seq2SeqEncoder\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules import Attention\n",
    "from allennlp.nn.beam_search import BeamSearch\n",
    "from allennlp.nn import util\n",
    "\n",
    "from collections import Counter\n",
    "from statistics import mean\n",
    "\n",
    "from true_rouge import Rouge\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.chrf_score import corpus_chrf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc295f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meteor:\n",
    "    def __init__(self, meteor_jar, language):\n",
    "        # Used to guarantee thread safety\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "        self.meteor_cmd = ['java', '-jar', '-Xmx2G', meteor_jar, '-', '-', '-stdio', '-l', language, '-norm']\n",
    "        self.meteor_p = subprocess.Popen(self.meteor_cmd,\n",
    "                                         stdin=subprocess.PIPE,\n",
    "                                         stdout=subprocess.PIPE,\n",
    "                                         stderr=subprocess.STDOUT,\n",
    "                                         encoding='utf-8',\n",
    "                                         bufsize=0)\n",
    "\n",
    "    def compute_score(self, hyps, refs):\n",
    "        scores = []\n",
    "        self.lock.acquire()\n",
    "        for hyp, ref in zip(hyps, refs):\n",
    "            stat = self._stat(hyp, ref)\n",
    "            # EVAL ||| stats\n",
    "            eval_line = 'EVAL ||| {}'.format(\" \".join(map(str, map(int, map(float, stat.split())))))\n",
    "            self.meteor_p.stdin.write('{}\\n'.format(eval_line))\n",
    "            scores.append(float(self.meteor_p.stdout.readline().strip()))\n",
    "        self.lock.release()\n",
    "\n",
    "        return sum(scores) / len(scores)\n",
    "\n",
    "    def _stat(self, hypothesis_str, reference_list):\n",
    "        # SCORE ||| reference 1 words ||| reference n words ||| hypothesis words\n",
    "        hypothesis_str = hypothesis_str.replace('|||', '').replace('  ', ' ')\n",
    "        score_line = ' ||| '.join(('SCORE', ' ||| '.join(reference_list), hypothesis_str))\n",
    "        self.meteor_p.stdin.write('{}\\n'.format(score_line))\n",
    "        return self.meteor_p.stdout.readline().strip()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.lock.acquire()\n",
    "        self.meteor_p.stdin.close()\n",
    "        self.meteor_p.kill()\n",
    "        self.meteor_p.wait()\n",
    "        self.lock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa805e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_duplicate_n_grams_rate(documents):\n",
    "    all_ngrams_count = Counter()\n",
    "    duplicate_ngrams_count = Counter()\n",
    "    for doc in documents:\n",
    "        words = doc.split(\" \")\n",
    "        for n in range(1, 5):\n",
    "            ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "            unique_ngrams = set(ngrams)\n",
    "            all_ngrams_count[n] += len(ngrams)\n",
    "            duplicate_ngrams_count[n] += len(ngrams) - len(unique_ngrams)\n",
    "    return {n: duplicate_ngrams_count[n]/all_ngrams_count[n] if all_ngrams_count[n] else 0.0\n",
    "            for n in range(1, 5)}\n",
    "\n",
    "\n",
    "def calc_bert_score(\n",
    "    hyps,\n",
    "    refs,\n",
    "    lang=\"ru\",\n",
    "    bert_score_model=None,\n",
    "    num_layers=None,\n",
    "    idf=False,\n",
    "    batch_size=32\n",
    "):\n",
    "    import bert_score\n",
    "    all_preds, hash_code = bert_score.score(\n",
    "        hyps,\n",
    "        refs,\n",
    "        lang=lang,\n",
    "        model_type=bert_score_model,\n",
    "        num_layers=num_layers,\n",
    "        verbose=False,\n",
    "        idf=idf,\n",
    "        batch_size=batch_size,\n",
    "        return_hash=True\n",
    "    )\n",
    "    avg_scores = [s.mean(dim=0) for s in all_preds]\n",
    "    return {\n",
    "        \"p\": avg_scores[0].cpu().item(),\n",
    "        \"r\": avg_scores[1].cpu().item(),\n",
    "        \"f\": avg_scores[2].cpu().item()\n",
    "    }, hash_code\n",
    "\n",
    "\n",
    "def calc_metrics(\n",
    "    refs, hyps,\n",
    "    language,\n",
    "    metric=\"all\",\n",
    "    meteor_jar=None\n",
    "):\n",
    "    metrics = dict()\n",
    "    metrics[\"count\"] = len(hyps)\n",
    "    metrics[\"ref_example\"] = refs[-1]\n",
    "    metrics[\"hyp_example\"] = hyps[-1]\n",
    "    many_refs = [[r] if r is not list else r for r in refs]\n",
    "    if metric in (\"bleu\", \"all\"):\n",
    "        t_hyps = [hyp.split(\" \") for hyp in hyps]\n",
    "        t_refs = [[r.split(\" \") for r in rs] for rs in many_refs]\n",
    "        metrics[\"bleu\"] = corpus_bleu(t_refs, t_hyps)\n",
    "    if metric in (\"rouge\", \"all\"):\n",
    "        rouge = Rouge()\n",
    "        scores = rouge.get_scores(hyps, refs, avg=True)\n",
    "        metrics.update(scores)\n",
    "    if metric in (\"meteor\", \"all\") and meteor_jar is not None and os.path.exists(meteor_jar):\n",
    "        meteor = Meteor(meteor_jar, language=language)\n",
    "        metrics[\"meteor\"] = meteor.compute_score(hyps, many_refs)\n",
    "    if metric in (\"duplicate_ngrams\", \"all\"):\n",
    "        metrics[\"duplicate_ngrams\"] = dict()\n",
    "        metrics[\"duplicate_ngrams\"].update(calc_duplicate_n_grams_rate(hyps))\n",
    "    if metric in (\"bert_score\",) and torch.cuda.is_available():\n",
    "        bert_scores, hash_code = calc_bert_score(hyps, refs)\n",
    "        metrics[\"bert_score_{}\".format(hash_code)] = bert_scores\n",
    "    if metric in (\"chrf\", \"all\"):\n",
    "        metrics[\"chrf\"] = corpus_chrf(refs, hyps, beta=1.0)\n",
    "    if metric in (\"length\", \"all\"):\n",
    "        metrics[\"length\"] = mean([len(h) for h in hyps])\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_metrics(refs, hyps, language, metric=\"all\", meteor_jar=None):\n",
    "    metrics = calc_metrics(refs, hyps, language=language, metric=metric, meteor_jar=meteor_jar)\n",
    "\n",
    "    print(\"-------------METRICS-------------\")\n",
    "    print(\"Count:\\t\", metrics[\"count\"])\n",
    "    print(\"Ref:\\t\", metrics[\"ref_example\"])\n",
    "    print(\"Hyp:\\t\", metrics[\"hyp_example\"])\n",
    "\n",
    "    if \"bleu\" in metrics:\n",
    "        print(\"BLEU:     \\t{:3.1f}\".format(metrics[\"bleu\"] * 100.0))\n",
    "    if \"chrf\" in metrics:\n",
    "        print(\"chrF:     \\t{:3.1f}\".format(metrics[\"chrf\"] * 100.0))\n",
    "    if \"rouge-1\" in metrics:\n",
    "        print(\"ROUGE-1-F:\\t{:3.1f}\".format(metrics[\"rouge-1\"]['f'] * 100.0))\n",
    "        print(\"ROUGE-2-F:\\t{:3.1f}\".format(metrics[\"rouge-2\"]['f'] * 100.0))\n",
    "        print(\"ROUGE-L-F:\\t{:3.1f}\".format(metrics[\"rouge-l\"]['f'] * 100.0))\n",
    "    if \"meteor\" in metrics:\n",
    "        print(\"METEOR:   \\t{:3.1f}\".format(metrics[\"meteor\"] * 100.0))\n",
    "    if \"duplicate_ngrams\" in metrics:\n",
    "        print(\"Dup 1-grams:\\t{:3.1f}\".format(metrics[\"duplicate_ngrams\"][1] * 100.0))\n",
    "        print(\"Dup 2-grams:\\t{:3.1f}\".format(metrics[\"duplicate_ngrams\"][2] * 100.0))\n",
    "        print(\"Dup 3-grams:\\t{:3.1f}\".format(metrics[\"duplicate_ngrams\"][3] * 100.0))\n",
    "    if \"length\" in metrics:\n",
    "        print(\"Avg length:\\t{:3.1f}\".format(metrics[\"length\"]))\n",
    "    for key, value in metrics.items():\n",
    "        if \"bert_score\" not in key:\n",
    "            continue\n",
    "        print(\"{}:\\t{:3.1f}\".format(key, value[\"f\"] * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e0b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_detokenize(text):\n",
    "    text = text.strip()\n",
    "    punctuation = \",.!?:;%\"\n",
    "    closing_punctuation = \")]}\"\n",
    "    opening_punctuation = \"([}\"\n",
    "    for ch in punctuation + closing_punctuation:\n",
    "        text = text.replace(\" \" + ch, ch)\n",
    "    for ch in opening_punctuation:\n",
    "        text = text.replace(ch + \" \", ch)\n",
    "    res = [r'\"\\s[^\"]+\\s\"', r\"'\\s[^']+\\s'\"]\n",
    "    for r in res:\n",
    "        for f in re.findall(r, text, re.U):\n",
    "            text = text.replace(f, f[0] + f[2:-2] + f[-1])\n",
    "    text = text.replace(\"' s\", \"'s\").replace(\" 's\", \"'s\")\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def postprocess(ref, hyp, language, is_multiple_ref=False, detokenize_after=False, tokenize_after=False, lower=False):\n",
    "    if is_multiple_ref:\n",
    "        reference_sents = ref.split(\" s_s \")\n",
    "        decoded_sents = hyp.split(\"s_s\")\n",
    "        hyp = [w.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").strip() for w in decoded_sents]\n",
    "        ref = [w.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").strip() for w in reference_sents]\n",
    "        hyp = \" \".join(hyp)\n",
    "        ref = \" \".join(ref)\n",
    "    ref = ref.strip()\n",
    "    hyp = hyp.strip()\n",
    "    if detokenize_after:\n",
    "        hyp = punct_detokenize(hyp)\n",
    "        ref = punct_detokenize(ref)\n",
    "    if tokenize_after:\n",
    "        hyp = hyp.replace(\"@@UNKNOWN@@\", \"<unk>\")\n",
    "        if language == \"ru\":\n",
    "            hyp = \" \".join([token.text for token in razdel.tokenize(hyp)])\n",
    "            ref = \" \".join([token.text for token in razdel.tokenize(ref)])\n",
    "        else:\n",
    "            hyp = \" \".join([token for token in nltk.word_tokenize(hyp)])\n",
    "            ref = \" \".join([token for token in nltk.word_tokenize(ref)])\n",
    "    if lower:\n",
    "        hyp = hyp.lower()\n",
    "        ref = ref.lower()\n",
    "    return ref, hyp\n",
    "\n",
    "\n",
    "def evaluate(predicted_path,\n",
    "             gold_path,\n",
    "             metric,\n",
    "             language,\n",
    "             max_count=None,\n",
    "             is_multiple_ref=False,\n",
    "             detokenize_after=False,\n",
    "             tokenize_after=False,\n",
    "             lower=False,\n",
    "             meteor_jar=None):\n",
    "    assert os.path.exists(gold_path)\n",
    "    assert os.path.exists(predicted_path)\n",
    "    if max_count is None:\n",
    "        with open(gold_path) as gold:\n",
    "            gold_num_lines = sum(1 for line in gold)\n",
    "        with open(predicted_path) as pred:\n",
    "            pred_num_lines = sum(1 for line in pred)\n",
    "        msg = \"Number of lines in files differ: {} vs {}\".format(gold_num_lines, pred_num_lines)\n",
    "        assert gold_num_lines == pred_num_lines, msg\n",
    "\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    with open(gold_path, \"r\") as gold, open(predicted_path, \"r\") as pred:\n",
    "        for i, (ref, hyp) in enumerate(zip(gold, pred)):\n",
    "            if max_count is not None and i >= max_count:\n",
    "                break\n",
    "            ref, hyp = postprocess(ref, hyp, language, is_multiple_ref, detokenize_after, tokenize_after, lower)\n",
    "            if not hyp:\n",
    "                print(\"Empty hyp for ref: \", ref)\n",
    "                continue\n",
    "            if not ref:\n",
    "                continue\n",
    "            refs.append(ref)\n",
    "            hyps.append(hyp)\n",
    "    print_metrics(refs, hyps, metric=metric, meteor_jar=meteor_jar, language=language)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93cb592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_method_score(records, predict_func, nrows=None, meteor_jar=\"meteor-1.5/meteor-1.5.jar\"):\n",
    "    references = []\n",
    "    predictions = []\n",
    "    for i, record in enumerate(records):\n",
    "        if nrows is not None and i >= nrows:\n",
    "            break\n",
    "        references.append(record[\"summary\"])\n",
    "        predictions.append(predict_func(record[\"text\"], record[\"summary\"]))\n",
    "\n",
    "    for i, (ref, hyp) in enumerate(zip(references, predictions)):\n",
    "        references[i], predictions[i] = postprocess(ref, hyp, language=\"ru\", tokenize_after=True, lower=True)\n",
    "    print_metrics(references, predictions, language=\"ru\", meteor_jar=meteor_jar)\n",
    "\n",
    "\n",
    "def calc_bert_score(records, predict_func, nrows=None):\n",
    "    references = []\n",
    "    predictions = []\n",
    "    for i, record in enumerate(records):\n",
    "        if nrows is not None and i >= nrows:\n",
    "            break\n",
    "        references.append(record[\"summary\"])\n",
    "        predictions.append(predict_func(record[\"text\"], record[\"summary\"]))\n",
    "\n",
    "    for i, (ref, hyp) in enumerate(zip(references, predictions)):\n",
    "        references[i], predictions[i] = postprocess(ref, hyp, language=\"ru\", tokenize_after=False, lower=False)\n",
    "    print_metrics(references, predictions, language=\"ru\", meteor_jar=None, metric=\"bert_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa.summarizer import summarize\n",
    "\n",
    "\n",
    "def predict_text_rank(text, summary, summary_part=0.1):\n",
    "    return summarize(text, ratio=summary_part, language='russian').replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bb9c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Model.register(\"pgn\")\n",
    "class PointerGeneratorNetwork(Model):\n",
    "    \"\"\"\n",
    "    Based on https://arxiv.org/pdf/1704.04368.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab: Vocabulary,\n",
    "                 source_embedder: TextFieldEmbedder,\n",
    "                 encoder: Seq2SeqEncoder,\n",
    "                 attention: Attention,\n",
    "                 max_decoding_steps: int,\n",
    "                 beam_size: int = None,\n",
    "                 target_namespace: str = \"tokens\",\n",
    "                 target_embedding_dim: int = None,\n",
    "                 scheduled_sampling_ratio: float = 0.,\n",
    "                 projection_dim: int = None,\n",
    "                 use_coverage: bool = False,\n",
    "                 coverage_shift: float = 0.,\n",
    "                 coverage_loss_weight: float = None,\n",
    "                 embed_attn_to_output: bool = False) -> None:\n",
    "        super(PointerGeneratorNetwork, self).__init__(vocab)\n",
    "\n",
    "        self._target_namespace = target_namespace\n",
    "        self._start_index = self.vocab.get_token_index(START_SYMBOL, target_namespace)\n",
    "        self._end_index = self.vocab.get_token_index(END_SYMBOL, target_namespace)\n",
    "        self._unk_index = self.vocab.get_token_index(DEFAULT_OOV_TOKEN, target_namespace)\n",
    "        self._vocab_size = self.vocab.get_vocab_size(target_namespace)\n",
    "        assert self._vocab_size > 2, \\\n",
    "            \"Target vocabulary is empty. Make sure 'target_namespace' option of the model is correct.\"\n",
    "\n",
    "        # Encoder\n",
    "        self._source_embedder = source_embedder\n",
    "        self._encoder = encoder\n",
    "        self._encoder_output_dim = self._encoder.get_output_dim()\n",
    "\n",
    "        # Decoder\n",
    "        self._target_embedding_dim = target_embedding_dim or source_embedder.get_output_dim()\n",
    "        self._num_classes = self.vocab.get_vocab_size(target_namespace)\n",
    "        self._target_embedder = Embedding(self._target_embedding_dim, self._num_classes)\n",
    "\n",
    "        self._decoder_input_dim = self._encoder_output_dim + self._target_embedding_dim\n",
    "        self._decoder_output_dim = self._encoder_output_dim\n",
    "        self._decoder_cell = LSTMCell(self._decoder_input_dim, self._decoder_output_dim)\n",
    "\n",
    "        self._projection_dim = projection_dim or self._source_embedder.get_output_dim()\n",
    "        hidden_projection_dim = self._decoder_output_dim if not embed_attn_to_output else self._decoder_output_dim * 2\n",
    "        self._hidden_projection_layer = Linear(hidden_projection_dim, self._projection_dim)\n",
    "        self._output_projection_layer = Linear(self._projection_dim, self._num_classes)\n",
    "\n",
    "        self._p_gen_layer = Linear(self._decoder_output_dim * 3 + self._decoder_input_dim, 1)\n",
    "        self._attention = attention\n",
    "        self._use_coverage = use_coverage\n",
    "        self._coverage_loss_weight = coverage_loss_weight\n",
    "        self._eps = 1e-31\n",
    "        self._embed_attn_to_output = embed_attn_to_output\n",
    "        self._coverage_shift = coverage_shift\n",
    "\n",
    "        # Metrics\n",
    "        self._p_gen_sum = 0.0\n",
    "        self._p_gen_iterations = 0\n",
    "        self._coverage_loss_sum = 0.0\n",
    "        self._coverage_iterations = 0\n",
    "\n",
    "        # Decoding\n",
    "        self._scheduled_sampling_ratio = scheduled_sampling_ratio\n",
    "        self._max_decoding_steps = max_decoding_steps\n",
    "        self._beam_search = BeamSearch(self._end_index, max_steps=max_decoding_steps, beam_size=beam_size or 1)\n",
    "\n",
    "    def forward(self,\n",
    "                source_tokens: Dict[str, Dict[str, torch.LongTensor]],\n",
    "                source_token_ids: torch.Tensor,\n",
    "                source_to_target: torch.LongTensor,\n",
    "                target_tokens: Dict[str, Dict[str, torch.LongTensor]] = None,\n",
    "                target_token_ids: torch.Tensor = None,\n",
    "                metadata=None) -> Dict[str, torch.Tensor]:\n",
    "        state = self._encode(source_tokens)\n",
    "        target_tokens_tensor = target_tokens[\"tokens\"][\"tokens\"].long() if target_tokens else None\n",
    "        extra_zeros, modified_source_tokens, modified_target_tokens = self._prepare(\n",
    "            source_to_target, source_token_ids, target_tokens_tensor, target_token_ids)\n",
    "\n",
    "        state[\"tokens\"] = modified_source_tokens\n",
    "        state[\"extra_zeros\"] = extra_zeros\n",
    "\n",
    "        output_dict = {}\n",
    "        if target_tokens:\n",
    "            state[\"target_tokens\"] = modified_target_tokens\n",
    "            state = self._init_decoder_state(state)\n",
    "            output_dict = self._forward_loop(state, target_tokens)\n",
    "        output_dict[\"metadata\"] = metadata\n",
    "        output_dict[\"source_to_target\"] = source_to_target\n",
    "\n",
    "        if not self.training:\n",
    "            state = self._init_decoder_state(state)\n",
    "            predictions = self._forward_beam_search(state)\n",
    "            output_dict.update(predictions)\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def _prepare(self,\n",
    "                 source_tokens: torch.LongTensor,\n",
    "                 source_token_ids: torch.Tensor,\n",
    "                 target_tokens: torch.LongTensor = None,\n",
    "                 target_token_ids: torch.Tensor = None):\n",
    "        batch_size = source_tokens.size(0)\n",
    "        source_max_length = source_tokens.size(1)\n",
    "\n",
    "        tokens = source_tokens\n",
    "        token_ids = source_token_ids.long()\n",
    "\n",
    "        # Concat target tokens if exist\n",
    "        if target_tokens is not None:\n",
    "            tokens = torch.cat((tokens, target_tokens), 1)\n",
    "            token_ids = torch.cat((token_ids, target_token_ids.long()), 1)\n",
    "\n",
    "        is_unk = torch.eq(tokens, self._unk_index).long()\n",
    "        # Create tensor with ids of unknown tokens only.\n",
    "        # Those ids are batch-local.\n",
    "        unk_only = token_ids * is_unk\n",
    "\n",
    "        # Recalculate batch-local ids to range [1, count_of_unique_unk_tokens].\n",
    "        # All known tokens have zero id.\n",
    "        unk_token_nums = token_ids.new_zeros((batch_size, token_ids.size(1)))\n",
    "        for i in range(batch_size):\n",
    "            unique = torch.unique(unk_only[i, :], return_inverse=True, sorted=True)[1]\n",
    "            unk_token_nums[i, :] = unique\n",
    "\n",
    "        # Replace DEFAULT_OOV_TOKEN id with new batch-local ids starting from vocab_size\n",
    "        # For example, if vocabulary size is 50000, the first unique unknown token will have 50000 index,\n",
    "        # the second will have 50001 index and so on.\n",
    "        tokens = tokens - tokens * is_unk + (self._vocab_size - 1) * is_unk + unk_token_nums\n",
    "\n",
    "        modified_target_tokens = None\n",
    "        modified_source_tokens = tokens\n",
    "        if target_tokens is not None:\n",
    "            # Remove target unknown tokens that do not exist in source tokens\n",
    "            max_source_num = torch.max(tokens[:, :source_max_length], dim=1)[0]\n",
    "            vocab_size = max_source_num.new_full((1,), self._vocab_size-1)\n",
    "            max_source_num = torch.max(max_source_num, other=vocab_size).unsqueeze(1).expand((-1, tokens.size(1)))\n",
    "            unk_target_tokens_mask = torch.gt(tokens, max_source_num).long()\n",
    "            tokens = tokens - tokens * unk_target_tokens_mask + self._unk_index * unk_target_tokens_mask\n",
    "            modified_target_tokens = tokens[:, source_max_length:]\n",
    "            modified_source_tokens = tokens[:, :source_max_length]\n",
    "\n",
    "        # Count unique unknown source tokens to create enough zeros for final distribution\n",
    "        source_unk_count = torch.max(unk_token_nums[:, :source_max_length])\n",
    "        extra_zeros = tokens.new_zeros((batch_size, source_unk_count), dtype=torch.float32)\n",
    "        return extra_zeros, modified_source_tokens, modified_target_tokens\n",
    "\n",
    "    def _encode(self, source_tokens: Dict[str, torch.LongTensor]) -> Dict[str, torch.Tensor]:\n",
    "        # shape: (batch_size, max_input_sequence_length, encoder_input_dim)\n",
    "        embedded_input = self._source_embedder.forward(source_tokens)\n",
    "        # shape: (batch_size, max_input_sequence_length)\n",
    "        source_mask = util.get_text_field_mask(source_tokens)\n",
    "        # shape: (batch_size, max_input_sequence_length, encoder_output_dim)\n",
    "        encoder_outputs = self._encoder.forward(embedded_input, source_mask)\n",
    "\n",
    "        return {\n",
    "                \"source_mask\": source_mask,\n",
    "                \"encoder_outputs\": encoder_outputs,\n",
    "        }\n",
    "\n",
    "    def _init_decoder_state(self, state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        batch_size = state[\"source_mask\"].size(0)\n",
    "        # shape: (batch_size, encoder_output_dim)\n",
    "        final_encoder_output = util.get_final_encoder_states(\n",
    "                state[\"encoder_outputs\"],\n",
    "                state[\"source_mask\"],\n",
    "                self._encoder.is_bidirectional())\n",
    "        # Initialize the decoder hidden state with the final output of the encoder.\n",
    "        # shape: (batch_size, decoder_output_dim)\n",
    "        state[\"decoder_hidden\"] = final_encoder_output\n",
    "\n",
    "        encoder_outputs = state[\"encoder_outputs\"]\n",
    "        state[\"decoder_context\"] = encoder_outputs.new_zeros(batch_size, self._decoder_output_dim)\n",
    "        if self._embed_attn_to_output:\n",
    "            state[\"attn_context\"] = encoder_outputs.new_zeros(encoder_outputs.size(0), encoder_outputs.size(2))\n",
    "        if self._use_coverage:\n",
    "            state[\"coverage\"] = encoder_outputs.new_zeros(batch_size, encoder_outputs.size(1))\n",
    "        return state\n",
    "\n",
    "    def _prepare_output_projections(self,\n",
    "                                    last_predictions: torch.Tensor,\n",
    "                                    state: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        # shape: (group_size, max_input_sequence_length, encoder_output_dim)\n",
    "        encoder_outputs = state[\"encoder_outputs\"]\n",
    "        # shape: (group_size, max_input_sequence_length)\n",
    "        source_mask = state[\"source_mask\"]\n",
    "        # shape: (group_size, decoder_output_dim)\n",
    "        decoder_hidden = state[\"decoder_hidden\"]\n",
    "        # shape: (group_size, decoder_output_dim)\n",
    "        decoder_context = state[\"decoder_context\"]\n",
    "        # shape: (group_size, decoder_output_dim)\n",
    "        attn_context = state.get(\"attn_context\", None)\n",
    "\n",
    "        is_unk = (last_predictions >= self._vocab_size).long()\n",
    "        last_predictions_fixed = last_predictions - last_predictions * is_unk + self._unk_index * is_unk\n",
    "        embedded_input = self._target_embedder(last_predictions_fixed)\n",
    "\n",
    "        coverage = state.get(\"coverage\", None)\n",
    "\n",
    "        def get_attention_context(decoder_hidden_inner):\n",
    "            if coverage is None:\n",
    "                attention_scores = self._attention(decoder_hidden_inner, encoder_outputs, source_mask)\n",
    "            else:\n",
    "                attention_scores = self._attention(decoder_hidden_inner, encoder_outputs, source_mask, coverage)\n",
    "            attention_context = util.weighted_sum(encoder_outputs, attention_scores)\n",
    "            return attention_scores, attention_context\n",
    "\n",
    "        if not self._embed_attn_to_output:\n",
    "            attn_scores, attn_context = get_attention_context(decoder_hidden)\n",
    "            decoder_input = torch.cat((attn_context, embedded_input), -1)\n",
    "            decoder_hidden, decoder_context = self._decoder_cell(decoder_input, (decoder_hidden, decoder_context))\n",
    "            projection = self._hidden_projection_layer(decoder_hidden)\n",
    "        else:\n",
    "            decoder_input = torch.cat((attn_context, embedded_input), -1)\n",
    "            decoder_hidden, decoder_context = self._decoder_cell(decoder_input, (decoder_hidden, decoder_context))\n",
    "            attn_scores, attn_context = get_attention_context(decoder_hidden)\n",
    "            projection = self._hidden_projection_layer(torch.cat((attn_context, decoder_hidden), -1))\n",
    "\n",
    "        output_projections = self._output_projection_layer(projection)\n",
    "        if self._use_coverage:\n",
    "            state[\"coverage\"] = coverage + attn_scores\n",
    "        state[\"decoder_input\"] = decoder_input\n",
    "        state[\"decoder_hidden\"] = decoder_hidden\n",
    "        state[\"decoder_context\"] = decoder_context\n",
    "        state[\"attn_scores\"] = attn_scores\n",
    "        state[\"attn_context\"] = attn_context\n",
    "\n",
    "        return output_projections, state\n",
    "\n",
    "    def _get_final_dist(self, state: Dict[str, torch.Tensor], output_projections):\n",
    "        attn_dist = state[\"attn_scores\"]\n",
    "        tokens = state[\"tokens\"]\n",
    "        extra_zeros = state[\"extra_zeros\"]\n",
    "        attn_context = state[\"attn_context\"]\n",
    "        decoder_input = state[\"decoder_input\"]\n",
    "        decoder_hidden = state[\"decoder_hidden\"]\n",
    "        decoder_context = state[\"decoder_context\"]\n",
    "\n",
    "        decoder_state = torch.cat((decoder_hidden, decoder_context), 1)\n",
    "        p_gen = self._p_gen_layer(torch.cat((attn_context, decoder_state, decoder_input), 1))\n",
    "        p_gen = torch.sigmoid(p_gen)\n",
    "        self._p_gen_sum += torch.mean(p_gen).item()\n",
    "        self._p_gen_iterations += 1\n",
    "\n",
    "        vocab_dist = F.softmax(output_projections, dim=-1)\n",
    "\n",
    "        vocab_dist = vocab_dist * p_gen\n",
    "        attn_dist = attn_dist * (1.0 - p_gen)\n",
    "        if extra_zeros.size(1) != 0:\n",
    "            vocab_dist = torch.cat((vocab_dist, extra_zeros), 1)\n",
    "        final_dist = vocab_dist.scatter_add(1, tokens, attn_dist)\n",
    "        normalization_factor = final_dist.sum(1, keepdim=True)\n",
    "        final_dist = final_dist / normalization_factor\n",
    "\n",
    "        return final_dist\n",
    "\n",
    "    def _forward_loop(self,\n",
    "                      state: Dict[str, torch.Tensor],\n",
    "                      target_tokens: Dict[str, Dict[str, torch.LongTensor]] = None) -> Dict[str, torch.Tensor]:\n",
    "        # shape: (batch_size, max_input_sequence_length)\n",
    "        source_mask = state[\"source_mask\"]\n",
    "        batch_size = source_mask.size(0)\n",
    "\n",
    "        num_decoding_steps = self._max_decoding_steps\n",
    "        if target_tokens:\n",
    "            # shape: (batch_size, max_target_sequence_length)\n",
    "            targets = target_tokens[\"tokens\"][\"tokens\"]\n",
    "            _, target_sequence_length = targets.size()\n",
    "            num_decoding_steps = target_sequence_length - 1\n",
    "\n",
    "        if self._use_coverage:\n",
    "            coverage_loss = source_mask.new_zeros(1, dtype=torch.float32)\n",
    "\n",
    "        last_predictions = state[\"tokens\"].new_full((batch_size,), fill_value=self._start_index)\n",
    "        step_proba: List[torch.Tensor] = []\n",
    "        step_predictions: List[torch.Tensor] = []\n",
    "        for timestep in range(num_decoding_steps):\n",
    "            if self.training and torch.rand(1).item() < self._scheduled_sampling_ratio:\n",
    "                input_choices = last_predictions\n",
    "            elif not target_tokens:\n",
    "                input_choices = last_predictions\n",
    "            else:\n",
    "                input_choices = targets[:, timestep]\n",
    "\n",
    "            if self._use_coverage:\n",
    "                old_coverage = state[\"coverage\"]\n",
    "\n",
    "            output_projections, state = self._prepare_output_projections(input_choices, state)\n",
    "            final_dist = self._get_final_dist(state, output_projections)\n",
    "            step_proba.append(final_dist)\n",
    "            last_predictions = torch.max(final_dist, 1)[1]\n",
    "            step_predictions.append(last_predictions.unsqueeze(1))\n",
    "\n",
    "            if self._use_coverage:\n",
    "                step_coverage_loss = torch.sum(torch.min(state[\"attn_scores\"], old_coverage), 1)\n",
    "                coverage_loss = coverage_loss + step_coverage_loss\n",
    "\n",
    "        # shape: (batch_size, num_decoding_steps)\n",
    "        predictions = torch.cat(step_predictions, 1)\n",
    "\n",
    "        output_dict = {\"predictions\": predictions}\n",
    "\n",
    "        if target_tokens:\n",
    "            # shape: (batch_size, num_decoding_steps, num_classes)\n",
    "            num_classes = step_proba[0].size(1)\n",
    "            proba = step_proba[0].new_zeros((batch_size, num_classes, len(step_proba)))\n",
    "            for i, p in enumerate(step_proba):\n",
    "                proba[:, :, i] = p\n",
    "\n",
    "            loss = self._get_loss(proba, state[\"target_tokens\"], self._eps)\n",
    "            if self._use_coverage:\n",
    "                coverage_loss = torch.mean(coverage_loss / num_decoding_steps)\n",
    "                self._coverage_loss_sum += coverage_loss.item()\n",
    "                self._coverage_iterations += 1\n",
    "                modified_coverage_loss = relu(coverage_loss - self._coverage_shift) + self._coverage_shift - 1.0\n",
    "                loss = loss + self._coverage_loss_weight * modified_coverage_loss\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_loss(proba: torch.LongTensor,\n",
    "                  targets: torch.LongTensor,\n",
    "                  eps: float) -> torch.Tensor:\n",
    "        targets = targets[:, 1:]\n",
    "        proba = torch.log(proba + eps)\n",
    "        loss = torch.nn.NLLLoss(ignore_index=0)(proba, targets)\n",
    "        return loss\n",
    "\n",
    "    def _forward_beam_search(self, state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        batch_size = state[\"tokens\"].size()[0]\n",
    "        start_predictions = state[\"tokens\"].new_full((batch_size,), fill_value=self._start_index)\n",
    "\n",
    "        # shape (all_top_k_predictions): (batch_size, beam_size, num_decoding_steps)\n",
    "        # shape (log_probabilities): (batch_size, beam_size)\n",
    "        all_top_k_predictions, log_probabilities = self._beam_search.search(\n",
    "            start_predictions, state, self.take_step)\n",
    "\n",
    "        output_dict = {\n",
    "            \"class_log_probabilities\": log_probabilities,\n",
    "            \"predictions\": all_top_k_predictions,\n",
    "        }\n",
    "        return output_dict\n",
    "\n",
    "    def take_step(self,\n",
    "                  last_predictions: torch.Tensor,\n",
    "                  state: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        # shape: (group_size, num_classes)\n",
    "        output_projections, state = self._prepare_output_projections(last_predictions, state)\n",
    "        final_dist = self._get_final_dist(state, output_projections)\n",
    "        log_probabilities = torch.log(final_dist + self._eps)\n",
    "        return log_probabilities, state\n",
    "\n",
    "    def make_output_human_readable(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        predicted_indices = output_dict[\"predictions\"]\n",
    "        if not isinstance(predicted_indices, np.ndarray):\n",
    "            predicted_indices = predicted_indices.detach().cpu().numpy()\n",
    "        all_predicted_tokens = []\n",
    "        all_meta = output_dict[\"metadata\"]\n",
    "        all_source_to_target = output_dict[\"source_to_target\"]\n",
    "        for (indices, metadata), source_to_target in zip(zip(predicted_indices, all_meta), all_source_to_target):\n",
    "            all_predicted_tokens.append(self._decode_sample(indices, metadata, source_to_target))\n",
    "        output_dict[\"predicted_tokens\"] = all_predicted_tokens\n",
    "        return output_dict\n",
    "\n",
    "    def _decode_sample(self, indices, metadata, source_to_target):\n",
    "        all_predicted_tokens = []\n",
    "        if len(indices.shape) == 1:\n",
    "            indices = [indices]\n",
    "        for sample_indices in indices:\n",
    "            sample_indices = list(sample_indices)\n",
    "            # Collect indices till the first end_symbol\n",
    "            if self._end_index in sample_indices:\n",
    "                sample_indices = sample_indices[:sample_indices.index(self._end_index)]\n",
    "            # Get all unknown tokens from source\n",
    "            original_source_tokens = metadata[\"source_tokens\"]\n",
    "            unk_tokens = list()\n",
    "            for i, token_vocab_index in enumerate(source_to_target):\n",
    "                if token_vocab_index != self._unk_index:\n",
    "                    continue\n",
    "                token = original_source_tokens[i]\n",
    "                if token in unk_tokens:\n",
    "                    continue\n",
    "                unk_tokens.append(token)\n",
    "            predicted_tokens = []\n",
    "            for token_vocab_index in sample_indices:\n",
    "                if token_vocab_index < self._vocab_size:\n",
    "                    token = self.vocab.get_token_from_index(token_vocab_index, namespace=self._target_namespace)\n",
    "                else:\n",
    "                    unk_number = token_vocab_index - self._vocab_size\n",
    "                    token = unk_tokens[unk_number]\n",
    "                predicted_tokens.append(token)\n",
    "            all_predicted_tokens.append(predicted_tokens)\n",
    "        return all_predicted_tokens\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        if not self._use_coverage:\n",
    "            return {}\n",
    "        avg_coverage_loss = 0.0\n",
    "        if self._coverage_iterations != 0:\n",
    "            avg_coverage_loss = self._coverage_loss_sum / self._coverage_iterations\n",
    "        avg_p_gen = self._p_gen_sum / self._p_gen_iterations if self._p_gen_iterations != 0 else 0.0\n",
    "        metrics = {\"coverage_loss\": avg_coverage_loss, \"p_gen\": avg_p_gen}\n",
    "        if reset:\n",
    "            self._p_gen_sum = 0.0\n",
    "            self._p_gen_iterations = 0\n",
    "            self._coverage_loss_sum = 0.0\n",
    "            self._coverage_iterations = 0\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe014ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Model.register(\"pgn\")\n",
    "class PointerGeneratorNetwork(Model):\n",
    "    \"\"\"\n",
    "    Based on https://arxiv.org/pdf/1704.04368.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab: Vocabulary,\n",
    "                 source_embedder: TextFieldEmbedder,\n",
    "                 encoder: Seq2SeqEncoder,\n",
    "                 attention: Attention,\n",
    "                 max_decoding_steps: int,\n",
    "                 beam_size: int = None,\n",
    "                 target_namespace: str = \"tokens\",\n",
    "                 target_embedding_dim: int = None,\n",
    "                 scheduled_sampling_ratio: float = 0.,\n",
    "                 projection_dim: int = None,\n",
    "                 use_coverage: bool = False,\n",
    "                 coverage_shift: float = 0.,\n",
    "                 coverage_loss_weight: float = None,\n",
    "                 embed_attn_to_output: bool = False) -> None:\n",
    "        super(PointerGeneratorNetwork, self).__init__(vocab)\n",
    "\n",
    "        self._target_namespace = target_namespace\n",
    "        self._start_index = self.vocab.get_token_index(START_SYMBOL, target_namespace)\n",
    "        self._end_index = self.vocab.get_token_index(END_SYMBOL, target_namespace)\n",
    "        self._unk_index = self.vocab.get_token_index(DEFAULT_OOV_TOKEN, target_namespace)\n",
    "        self._vocab_size = self.vocab.get_vocab_size(target_namespace)\n",
    "        assert self._vocab_size > 2, \\\n",
    "            \"Target vocabulary is empty. Make sure 'target_namespace' option of the model is correct.\"\n",
    "\n",
    "        # Encoder\n",
    "        self._source_embedder = source_embedder\n",
    "        self._encoder = encoder\n",
    "        self._encoder_output_dim = self._encoder.get_output_dim()\n",
    "\n",
    "        # Decoder\n",
    "        self._target_embedding_dim = target_embedding_dim or source_embedder.get_output_dim()\n",
    "        self._num_classes = self.vocab.get_vocab_size(target_namespace)\n",
    "        self._target_embedder = Embedding(self._target_embedding_dim, self._num_classes)\n",
    "\n",
    "        self._decoder_input_dim = self._encoder_output_dim + self._target_embedding_dim\n",
    "        self._decoder_output_dim = self._encoder_output_dim\n",
    "        self._decoder_cell = LSTMCell(self._decoder_input_dim, self._decoder_output_dim)\n",
    "\n",
    "        self._projection_dim = projection_dim or self._source_embedder.get_output_dim()\n",
    "        hidden_projection_dim = self._decoder_output_dim if not embed_attn_to_output else self._decoder_output_dim * 2\n",
    "        self._hidden_projection_layer = Linear(hidden_projection_dim, self._projection_dim)\n",
    "        self._output_projection_layer = Linear(self._projection_dim, self._num_classes)\n",
    "\n",
    "        self._p_gen_layer = Linear(self._decoder_output_dim * 3 + self._decoder_input_dim, 1)\n",
    "        self._attention = attention\n",
    "        self._use_coverage = use_coverage\n",
    "        self._coverage_loss_weight = coverage_loss_weight\n",
    "        self._eps = 1e-31\n",
    "        self._embed_attn_to_output = embed_attn_to_output\n",
    "        self._coverage_shift = coverage_shift\n",
    "\n",
    "        # Metrics\n",
    "        self._p_gen_sum = 0.0\n",
    "        self._p_gen_iterations = 0\n",
    "        self._coverage_loss_sum = 0.0\n",
    "        self._coverage_iterations = 0\n",
    "\n",
    "        # Decoding\n",
    "        self._scheduled_sampling_ratio = scheduled_sampling_ratio\n",
    "        self._max_decoding_steps = max_decoding_steps\n",
    "        self._beam_search = BeamSearch(self._end_index, max_steps=max_decoding_steps, beam_size=beam_size or 1)\n",
    "\n",
    "    def forward(self,\n",
    "                source_tokens: Dict[str, Dict[str, torch.LongTensor]],\n",
    "                source_token_ids: torch.Tensor,\n",
    "                source_to_target: torch.LongTensor,\n",
    "                target_tokens: Dict[str, Dict[str, torch.LongTensor]] = None,\n",
    "                target_token_ids: torch.Tensor = None,\n",
    "                metadata=None) -> Dict[str, torch.Tensor]:\n",
    "        state = self._encode(source_tokens)\n",
    "        target_tokens_tensor = target_tokens[\"tokens\"][\"tokens\"].long() if target_tokens else None\n",
    "        extra_zeros, modified_source_tokens, modified_target_tokens = self._prepare(\n",
    "            source_to_target, source_token_ids, target_tokens_tensor, target_token_ids)\n",
    "\n",
    "        state[\"tokens\"] = modified_source_tokens\n",
    "        state[\"extra_zeros\"] = extra_zeros\n",
    "\n",
    "        output_dict = {}\n",
    "        if target_tokens:\n",
    "            state[\"target_tokens\"] = modified_target_tokens\n",
    "            state = self._init_decoder_state(state)\n",
    "            output_dict = self._forward_loop(state, target_tokens)\n",
    "        output_dict[\"metadata\"] = metadata\n",
    "        output_dict[\"source_to_target\"] = source_to_target\n",
    "\n",
    "        if not self.training:\n",
    "            state = self._init_decoder_state(state)\n",
    "            predictions = self._forward_beam_search(state)\n",
    "            output_dict.update(predictions)\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def _prepare(self,\n",
    "                 source_tokens: torch.LongTensor,\n",
    "                 source_token_ids: torch.Tensor,\n",
    "                 target_tokens: torch.LongTensor = None,\n",
    "                 target_token_ids: torch.Tensor = None):\n",
    "        batch_size = source_tokens.size(0)\n",
    "        source_max_length = source_tokens.size(1)\n",
    "\n",
    "        tokens = source_tokens\n",
    "        token_ids = source_token_ids.long()\n",
    "\n",
    "        # Concat target tokens if exist\n",
    "        if target_tokens is not None:\n",
    "            tokens = torch.cat((tokens, target_tokens), 1)\n",
    "            token_ids = torch.cat((token_ids, target_token_ids.long()), 1)\n",
    "\n",
    "        is_unk = torch.eq(tokens, self._unk_index).long()\n",
    "        # Create tensor with ids of unknown tokens only.\n",
    "        # Those ids are batch-local.\n",
    "        unk_only = token_ids * is_unk\n",
    "\n",
    "        # Recalculate batch-local ids to range [1, count_of_unique_unk_tokens].\n",
    "        # All known tokens have zero id.\n",
    "        unk_token_nums = token_ids.new_zeros((batch_size, token_ids.size(1)))\n",
    "        for i in range(batch_size):\n",
    "            unique = torch.unique(unk_only[i, :], return_inverse=True, sorted=True)[1]\n",
    "            unk_token_nums[i, :] = unique\n",
    "\n",
    "        # Replace DEFAULT_OOV_TOKEN id with new batch-local ids starting from vocab_size\n",
    "        # For example, if vocabulary size is 50000, the first unique unknown token will have 50000 index,\n",
    "        # the second will have 50001 index and so on.\n",
    "        tokens = tokens - tokens * is_unk + (self._vocab_size - 1) * is_unk + unk_token_nums\n",
    "\n",
    "        modified_target_tokens = None\n",
    "        modified_source_tokens = tokens\n",
    "        if target_tokens is not None:\n",
    "            # Remove target unknown tokens that do not exist in source tokens\n",
    "            max_source_num = torch.max(tokens[:, :source_max_length], dim=1)[0]\n",
    "            vocab_size = max_source_num.new_full((1,), self._vocab_size-1)\n",
    "            max_source_num = torch.max(max_source_num, other=vocab_size).unsqueeze(1).expand((-1, tokens.size(1)))\n",
    "            unk_target_tokens_mask = torch.gt(tokens, max_source_num).long()\n",
    "            tokens = tokens - tokens * unk_target_tokens_mask + self._unk_index * unk_target_tokens_mask\n",
    "            modified_target_tokens = tokens[:, source_max_length:]\n",
    "            modified_source_tokens = tokens[:, :source_max_length]\n",
    "\n",
    "        # Count unique unknown source tokens to create enough zeros for final distribution\n",
    "        source_unk_count = torch.max(unk_token_nums[:, :source_max_length])\n",
    "        extra_zeros = tokens.new_zeros((batch_size, source_unk_count), dtype=torch.float32)\n",
    "        return extra_zeros, modified_source_tokens, modified_target_tokens\n",
    "\n",
    "    def _encode(self, source_tokens: Dict[str, torch.LongTensor]) -> Dict[str, torch.Tensor]:\n",
    "        # shape: (batch_size, max_input_sequence_length, encoder_input_dim)\n",
    "        embedded_input = self._source_embedder.forward(source_tokens)\n",
    "        # shape: (batch_size, max_input_sequence_length)\n",
    "        source_mask = util.get_text_field_mask(source_tokens)\n",
    "        # shape: (batch_size, max_input_sequence_length, encoder_output_dim)\n",
    "        encoder_outputs = self._encoder.forward(embedded_input, source_mask)\n",
    "\n",
    "        return {\n",
    "                \"source_mask\": source_mask,\n",
    "                \"encoder_outputs\": encoder_outputs,\n",
    "        }\n",
    "\n",
    "    def _init_decoder_state(self, state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        batch_size = state[\"source_mask\"].size(0)\n",
    "        # shape: (batch_size, encoder_output_dim)\n",
    "        final_encoder_output = util.get_final_encoder_states(\n",
    "                state[\"encoder_outputs\"],\n",
    "                state[\"source_mask\"],\n",
    "                self._encoder.is_bidirectional())\n",
    "        # Initialize the decoder hidden state with the final output of the encoder.\n",
    "        # shape: (batch_size, decoder_output_dim)\n",
    "        state[\"decoder_hidden\"] = final_encoder_output\n",
    "\n",
    "        encoder_outputs = state[\"encoder_outputs\"]\n",
    "        state[\"decoder_context\"] = encoder_outputs.new_zeros(batch_size, self._decoder_output_dim)\n",
    "        if self._embed_attn_to_output:\n",
    "            state[\"attn_context\"] = encoder_outputs.new_zeros(encoder_outputs.size(0), encoder_outputs.size(2))\n",
    "        if self._use_coverage:\n",
    "            state[\"coverage\"] = encoder_outputs.new_zeros(batch_size, encoder_outputs.size(1))\n",
    "        return state\n",
    "\n",
    "    def _prepare_output_projections(self,\n",
    "                                    last_predictions: torch.Tensor,\n",
    "                                    state: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        # shape: (group_size, max_input_sequence_length, encoder_output_dim)\n",
    "        encoder_outputs = state[\"encoder_outputs\"]\n",
    "        # shape: (group_size, max_input_sequence_length)\n",
    "        source_mask = state[\"source_mask\"]\n",
    "        # shape: (group_size, decoder_output_dim)\n",
    "        decoder_hidden = state[\"decoder_hidden\"]\n",
    "        # shape: (group_size, decoder_output_dim)\n",
    "        decoder_context = state[\"decoder_context\"]\n",
    "        # shape: (group_size, decoder_output_dim)\n",
    "        attn_context = state.get(\"attn_context\", None)\n",
    "\n",
    "        is_unk = (last_predictions >= self._vocab_size).long()\n",
    "        last_predictions_fixed = last_predictions - last_predictions * is_unk + self._unk_index * is_unk\n",
    "        embedded_input = self._target_embedder(last_predictions_fixed)\n",
    "\n",
    "        coverage = state.get(\"coverage\", None)\n",
    "\n",
    "        def get_attention_context(decoder_hidden_inner):\n",
    "            if coverage is None:\n",
    "                attention_scores = self._attention(decoder_hidden_inner, encoder_outputs, source_mask)\n",
    "            else:\n",
    "                attention_scores = self._attention(decoder_hidden_inner, encoder_outputs, source_mask, coverage)\n",
    "            attention_context = util.weighted_sum(encoder_outputs, attention_scores)\n",
    "            return attention_scores, attention_context\n",
    "\n",
    "        if not self._embed_attn_to_output:\n",
    "            attn_scores, attn_context = get_attention_context(decoder_hidden)\n",
    "            decoder_input = torch.cat((attn_context, embedded_input), -1)\n",
    "            decoder_hidden, decoder_context = self._decoder_cell(decoder_input, (decoder_hidden, decoder_context))\n",
    "            projection = self._hidden_projection_layer(decoder_hidden)\n",
    "        else:\n",
    "            decoder_input = torch.cat((attn_context, embedded_input), -1)\n",
    "            decoder_hidden, decoder_context = self._decoder_cell(decoder_input, (decoder_hidden, decoder_context))\n",
    "            attn_scores, attn_context = get_attention_context(decoder_hidden)\n",
    "            projection = self._hidden_projection_layer(torch.cat((attn_context, decoder_hidden), -1))\n",
    "\n",
    "        output_projections = self._output_projection_layer(projection)\n",
    "        if self._use_coverage:\n",
    "            state[\"coverage\"] = coverage + attn_scores\n",
    "        state[\"decoder_input\"] = decoder_input\n",
    "        state[\"decoder_hidden\"] = decoder_hidden\n",
    "        state[\"decoder_context\"] = decoder_context\n",
    "        state[\"attn_scores\"] = attn_scores\n",
    "        state[\"attn_context\"] = attn_context\n",
    "\n",
    "        return output_projections, state\n",
    "\n",
    "    def _get_final_dist(self, state: Dict[str, torch.Tensor], output_projections):\n",
    "        attn_dist = state[\"attn_scores\"]\n",
    "        tokens = state[\"tokens\"]\n",
    "        extra_zeros = state[\"extra_zeros\"]\n",
    "        attn_context = state[\"attn_context\"]\n",
    "        decoder_input = state[\"decoder_input\"]\n",
    "        decoder_hidden = state[\"decoder_hidden\"]\n",
    "        decoder_context = state[\"decoder_context\"]\n",
    "\n",
    "        decoder_state = torch.cat((decoder_hidden, decoder_context), 1)\n",
    "        p_gen = self._p_gen_layer(torch.cat((attn_context, decoder_state, decoder_input), 1))\n",
    "        p_gen = torch.sigmoid(p_gen)\n",
    "        self._p_gen_sum += torch.mean(p_gen).item()\n",
    "        self._p_gen_iterations += 1\n",
    "\n",
    "        vocab_dist = F.softmax(output_projections, dim=-1)\n",
    "\n",
    "        vocab_dist = vocab_dist * p_gen\n",
    "        attn_dist = attn_dist * (1.0 - p_gen)\n",
    "        if extra_zeros.size(1) != 0:\n",
    "            vocab_dist = torch.cat((vocab_dist, extra_zeros), 1)\n",
    "        final_dist = vocab_dist.scatter_add(1, tokens, attn_dist)\n",
    "        normalization_factor = final_dist.sum(1, keepdim=True)\n",
    "        final_dist = final_dist / normalization_factor\n",
    "\n",
    "        return final_dist\n",
    "\n",
    "    def _forward_loop(self,\n",
    "                      state: Dict[str, torch.Tensor],\n",
    "                      target_tokens: Dict[str, Dict[str, torch.LongTensor]] = None) -> Dict[str, torch.Tensor]:\n",
    "        # shape: (batch_size, max_input_sequence_length)\n",
    "        source_mask = state[\"source_mask\"]\n",
    "        batch_size = source_mask.size(0)\n",
    "\n",
    "        num_decoding_steps = self._max_decoding_steps\n",
    "        if target_tokens:\n",
    "            # shape: (batch_size, max_target_sequence_length)\n",
    "            targets = target_tokens[\"tokens\"][\"tokens\"]\n",
    "            _, target_sequence_length = targets.size()\n",
    "            num_decoding_steps = target_sequence_length - 1\n",
    "\n",
    "        if self._use_coverage:\n",
    "            coverage_loss = source_mask.new_zeros(1, dtype=torch.float32)\n",
    "\n",
    "        last_predictions = state[\"tokens\"].new_full((batch_size,), fill_value=self._start_index)\n",
    "        step_proba: List[torch.Tensor] = []\n",
    "        step_predictions: List[torch.Tensor] = []\n",
    "        for timestep in range(num_decoding_steps):\n",
    "            if self.training and torch.rand(1).item() < self._scheduled_sampling_ratio:\n",
    "                input_choices = last_predictions\n",
    "            elif not target_tokens:\n",
    "                input_choices = last_predictions\n",
    "            else:\n",
    "                input_choices = targets[:, timestep]\n",
    "\n",
    "            if self._use_coverage:\n",
    "                old_coverage = state[\"coverage\"]\n",
    "\n",
    "            output_projections, state = self._prepare_output_projections(input_choices, state)\n",
    "            final_dist = self._get_final_dist(state, output_projections)\n",
    "            step_proba.append(final_dist)\n",
    "            last_predictions = torch.max(final_dist, 1)[1]\n",
    "            step_predictions.append(last_predictions.unsqueeze(1))\n",
    "\n",
    "            if self._use_coverage:\n",
    "                step_coverage_loss = torch.sum(torch.min(state[\"attn_scores\"], old_coverage), 1)\n",
    "                coverage_loss = coverage_loss + step_coverage_loss\n",
    "\n",
    "        # shape: (batch_size, num_decoding_steps)\n",
    "        predictions = torch.cat(step_predictions, 1)\n",
    "\n",
    "        output_dict = {\"predictions\": predictions}\n",
    "\n",
    "        if target_tokens:\n",
    "            # shape: (batch_size, num_decoding_steps, num_classes)\n",
    "            num_classes = step_proba[0].size(1)\n",
    "            proba = step_proba[0].new_zeros((batch_size, num_classes, len(step_proba)))\n",
    "            for i, p in enumerate(step_proba):\n",
    "                proba[:, :, i] = p\n",
    "\n",
    "            loss = self._get_loss(proba, state[\"target_tokens\"], self._eps)\n",
    "            if self._use_coverage:\n",
    "                coverage_loss = torch.mean(coverage_loss / num_decoding_steps)\n",
    "                self._coverage_loss_sum += coverage_loss.item()\n",
    "                self._coverage_iterations += 1\n",
    "                modified_coverage_loss = relu(coverage_loss - self._coverage_shift) + self._coverage_shift - 1.0\n",
    "                loss = loss + self._coverage_loss_weight * modified_coverage_loss\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_loss(proba: torch.LongTensor,\n",
    "                  targets: torch.LongTensor,\n",
    "                  eps: float) -> torch.Tensor:\n",
    "        targets = targets[:, 1:]\n",
    "        proba = torch.log(proba + eps)\n",
    "        loss = torch.nn.NLLLoss(ignore_index=0)(proba, targets)\n",
    "        return loss\n",
    "\n",
    "    def _forward_beam_search(self, state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        batch_size = state[\"tokens\"].size()[0]\n",
    "        start_predictions = state[\"tokens\"].new_full((batch_size,), fill_value=self._start_index)\n",
    "\n",
    "        # shape (all_top_k_predictions): (batch_size, beam_size, num_decoding_steps)\n",
    "        # shape (log_probabilities): (batch_size, beam_size)\n",
    "        all_top_k_predictions, log_probabilities = self._beam_search.search(\n",
    "            start_predictions, state, self.take_step)\n",
    "\n",
    "        output_dict = {\n",
    "            \"class_log_probabilities\": log_probabilities,\n",
    "            \"predictions\": all_top_k_predictions,\n",
    "        }\n",
    "        return output_dict\n",
    "\n",
    "    def take_step(self,\n",
    "                  last_predictions: torch.Tensor,\n",
    "                  state: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        # shape: (group_size, num_classes)\n",
    "        output_projections, state = self._prepare_output_projections(last_predictions, state)\n",
    "        final_dist = self._get_final_dist(state, output_projections)\n",
    "        log_probabilities = torch.log(final_dist + self._eps)\n",
    "        return log_probabilities, state\n",
    "\n",
    "    def make_output_human_readable(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        predicted_indices = output_dict[\"predictions\"]\n",
    "        if not isinstance(predicted_indices, np.ndarray):\n",
    "            predicted_indices = predicted_indices.detach().cpu().numpy()\n",
    "        all_predicted_tokens = []\n",
    "        all_meta = output_dict[\"metadata\"]\n",
    "        all_source_to_target = output_dict[\"source_to_target\"]\n",
    "        for (indices, metadata), source_to_target in zip(zip(predicted_indices, all_meta), all_source_to_target):\n",
    "            all_predicted_tokens.append(self._decode_sample(indices, metadata, source_to_target))\n",
    "        output_dict[\"predicted_tokens\"] = all_predicted_tokens\n",
    "        return output_dict\n",
    "\n",
    "    def _decode_sample(self, indices, metadata, source_to_target):\n",
    "        all_predicted_tokens = []\n",
    "        if len(indices.shape) == 1:\n",
    "            indices = [indices]\n",
    "        for sample_indices in indices:\n",
    "            sample_indices = list(sample_indices)\n",
    "            # Collect indices till the first end_symbol\n",
    "            if self._end_index in sample_indices:\n",
    "                sample_indices = sample_indices[:sample_indices.index(self._end_index)]\n",
    "            # Get all unknown tokens from source\n",
    "            original_source_tokens = metadata[\"source_tokens\"]\n",
    "            unk_tokens = list()\n",
    "            for i, token_vocab_index in enumerate(source_to_target):\n",
    "                if token_vocab_index != self._unk_index:\n",
    "                    continue\n",
    "                token = original_source_tokens[i]\n",
    "                if token in unk_tokens:\n",
    "                    continue\n",
    "                unk_tokens.append(token)\n",
    "            predicted_tokens = []\n",
    "            for token_vocab_index in sample_indices:\n",
    "                if token_vocab_index < self._vocab_size:\n",
    "                    token = self.vocab.get_token_from_index(token_vocab_index, namespace=self._target_namespace)\n",
    "                else:\n",
    "                    unk_number = token_vocab_index - self._vocab_size\n",
    "                    token = unk_tokens[unk_number]\n",
    "                predicted_tokens.append(token)\n",
    "            all_predicted_tokens.append(predicted_tokens)\n",
    "        return all_predicted_tokens\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        if not self._use_coverage:\n",
    "            return {}\n",
    "        avg_coverage_loss = 0.0\n",
    "        if self._coverage_iterations != 0:\n",
    "            avg_coverage_loss = self._coverage_loss_sum / self._coverage_iterations\n",
    "        avg_p_gen = self._p_gen_sum / self._p_gen_iterations if self._p_gen_iterations != 0 else 0.0\n",
    "        metrics = {\"coverage_loss\": avg_coverage_loss, \"p_gen\": avg_p_gen}\n",
    "        if reset:\n",
    "            self._p_gen_sum = 0.0\n",
    "            self._p_gen_iterations = 0\n",
    "            self._coverage_loss_sum = 0.0\n",
    "            self._coverage_iterations = 0\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76652424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937ae49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Model.register(\"pgn_textrank\")\n",
    "class PointerGeneratorNetworkTextRank(PointerGeneratorNetwork):\n",
    "    \"\"\"\n",
    "    Based on https://link.springer.com/chapter/10.1007/978-3-319-99495-6_39\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab: Vocabulary,\n",
    "                 source_embedder: TextFieldEmbedder,\n",
    "                 encoder: Seq2SeqEncoder,\n",
    "                 attention: Attention,\n",
    "                 max_decoding_steps: int,\n",
    "                 beam_size: int = None,\n",
    "                 target_namespace: str = \"tokens\",\n",
    "                 target_embedding_dim: int = None,\n",
    "                 scheduled_sampling_ratio: float = 0.,\n",
    "                 projection_dim: int = None,\n",
    "                 use_coverage: bool = False,\n",
    "                 coverage_shift: float = 0.,\n",
    "                 coverage_loss_weight: float = None,\n",
    "                 embed_attn_to_output: bool = False) -> None:\n",
    "        super(PointerGeneratorNetwork, self).__init__(\n",
    "            vocab, \n",
    "            source_embedder,\n",
    "            encoder,\n",
    "            attention,\n",
    "            max_decoding_steps,\n",
    "            beam_size,\n",
    "            target_namespace,\n",
    "            target_embedding_dim,\n",
    "            scheduled_sampling_ratio,\n",
    "            projection_dim,\n",
    "            use_coverage,\n",
    "            coverage_shift,\n",
    "            coverage_loss_weight,\n",
    "            embed_attn_to_output\n",
    "        )\n",
    "        self.vocab = self.predict_text_rank(self.vocab)\n",
    "        def predict_text_rank(self, text, summary, summary_part=0.1):\n",
    "            return summarize(text, ratio=summary_part, language='russian').replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.token_class import Token\n",
    "from allennlp.data.tokenizers.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "@Tokenizer.register(\"razdel\")\n",
    "class RazdelTokenizer(Tokenizer):\n",
    "    def __init__(self, lowercase: bool = False):\n",
    "        self._lowercase = lowercase\n",
    "\n",
    "    def tokenize(self, text: str) -> List[Token]:\n",
    "        return [Token(token.text.lower()) if self._lowercase else Token(token.text) for token in razdel.tokenize(text)]\n",
    "\n",
    "    def batch_tokenize(self, texts: List[str]) -> List[List[Token]]:\n",
    "        return [self.tokenize(text) for text in texts]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
